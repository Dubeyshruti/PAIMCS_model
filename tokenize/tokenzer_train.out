sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=../../tokenizer_train_input.txt --model_prefix=paimcs_tokenizer --vocab_size=2673 --model_type=unigram --character_coverage=1.0 --pad_id=3 --bos_id=1 --eos_id=2 --unk_id=0 --max_sentence_length=10000
sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : 
trainer_spec {
  input: ../../tokenizer_train_input.txt
  input_format: 
  model_prefix: paimcs_tokenizer
  model_type: UNIGRAM
  vocab_size: 2673
  self_test_sample_size: 0
  character_coverage: 1
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 10000
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  pretokenization_delimiter: 
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  seed_sentencepieces_file: 
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: 3
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ⁇ 
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(185) LOG(INFO) Loading corpus: ../../tokenizer_train_input.txt
trainer_interface.cc(409) LOG(INFO) Loaded all 220 sentences
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>
trainer_interface.cc(430) LOG(INFO) Normalizing sentences...
trainer_interface.cc(539) LOG(INFO) all chars count=230528
trainer_interface.cc(560) LOG(INFO) Alphabet size=107
trainer_interface.cc(561) LOG(INFO) Final character coverage=1
trainer_interface.cc(592) LOG(INFO) Done! preprocessed 220 sentences.
unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=122800
unigram_model_trainer.cc(312) LOG(INFO) Initialized 16177 seed sentencepieces
trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 220
trainer_interface.cc(609) LOG(INFO) Done! 7824
unigram_model_trainer.cc(602) LOG(INFO) Using 7824 sentences for EM training
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5722 obj=11.7544 num_tokens=16188 num_tokens/piece=2.82908
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5006 obj=9.65778 num_tokens=16341 num_tokens/piece=3.26428
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3753 obj=9.73196 num_tokens=17713 num_tokens/piece=4.71969
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3745 obj=9.64822 num_tokens=17713 num_tokens/piece=4.72977
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2940 obj=9.90737 num_tokens=19468 num_tokens/piece=6.62177
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2940 obj=9.82965 num_tokens=19474 num_tokens/piece=6.62381
trainer_interface.cc(687) LOG(INFO) Saving model: paimcs_tokenizer.model
trainer_interface.cc(699) LOG(INFO) Saving vocabs: paimcs_tokenizer.vocab
Trained SentencePiece model: paimcs_tokenizer.model / paimcs_tokenizer.vocab
IDs:    [17, 1637, 103, 82, 94, 878, 21, 1127, 289, 1927, 25, 69, 5, 136, 4, 457, 502, 22, 594, 145, 7]
Pieces: ['▁The', '▁qu', 'ic', 'k', '▁b', 'row', 'n', '▁fo', 'x', '▁ju', 'm', 'p', 's', '▁over', '▁the', '▁la', 'z', 'y', '▁do', 'g', '.']
Decoded: The quick brown fox jumps over the lazy dog.
