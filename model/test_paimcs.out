2025-04-04 18:51:04.504293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743792664.568443    6979 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743792664.588309    6979 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-04 18:51:04.655007: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-04 18:51:14.060988: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
...WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1743792676.096943    6979 service.cc:148] XLA service 0xad86460 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1743792676.097174    6979 service.cc:156]   StreamExecutor device (0): Host, Default Version
I0000 00:00:1743792676.459712    6979 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
.WARNING:tensorflow:5 out of the last 5 calls to <function conv.<locals>._conv_xla at 0x1370165dad40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function conv.<locals>._conv_xla at 0x137016591580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
..........WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.
I0000 00:00:1743792713.602930    6979 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1743792713.603259    6979 single_machine.cc:361] Starting new session
W0000 00:00:1743792717.805014    6979 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1743792717.805091    6979 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
2025-04-04 18:52:02.700284: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexAddV2, FlexBatchMatMulV2, FlexBiasAdd, FlexConcatV2, FlexConv2D, FlexCos, FlexExp, FlexGatherV2, FlexMatMul, FlexMean, FlexMul, FlexRealDiv, FlexRsqrt, FlexSigmoid, FlexSplit, FlexSquare, FlexStridedSlice, FlexSub, FlexSum, FlexTranspose
Details:
	tf.AddV2(tensor<?x3x678x1xf16>, tensor<f16>) -> (tensor<?x3x678x1xf16>) : {device = ""}
	tf.AddV2(tensor<?x3x678x60xf16>, tensor<?x3x678x60xf16>) -> (tensor<?x3x678x60xf16>) : {device = ""}
	tf.AddV2(tensor<?x3x678x87xf16>, tensor<87xf16>) -> (tensor<?x3x678x87xf16>) : {device = ""}
	tf.AddV2(tensor<?x678x120xf16>, tensor<120xf16>) -> (tensor<?x678x120xf16>) : {device = ""}
	tf.AddV2(tensor<?x678x1xf16>, tensor<f16>) -> (tensor<?x678x1xf16>) : {device = ""}
	tf.AddV2(tensor<?x678x339xf16>, tensor<1x678x339xf16>) -> (tensor<?x678x339xf16>) : {device = ""}
	tf.AddV2(tensor<?x678x360xf16>, tensor<?x678x360xf16>) -> (tensor<?x678x360xf16>) : {device = ""}
	tf.BatchMatMulV2(tensor<?x3x678x120xf16>, tensor<120x87xf16>) -> (tensor<?x3x678x87xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BatchMatMulV2(tensor<?x3x678x87xf16>, tensor<?x3x87x120xf16>) -> (tensor<?x3x678x120xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BatchMatMulV2(tensor<?x3x678x87xf16>, tensor<?x3x87x1xf16>) -> (tensor<?x3x678x1xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BatchMatMulV2(tensor<?x3x87x678xf16>, tensor<?x3x678x120xf16>) -> (tensor<?x3x87x120xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BatchMatMulV2(tensor<?x678x1080xf16>, tensor<1080x360xf16>) -> (tensor<?x678x360xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BatchMatMulV2(tensor<?x678x339xf16>, tensor<339x120xf16>) -> (tensor<?x678x120xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BatchMatMulV2(tensor<?x678x360xf16>, tensor<360x1080xf16>) -> (tensor<?x678x1080xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BatchMatMulV2(tensor<?x678x360xf16>, tensor<360x120xf16>) -> (tensor<?x678x120xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BatchMatMulV2(tensor<?x678x360xf16>, tensor<360x360xf16>) -> (tensor<?x678x360xf16>) : {adj_x = false, adj_y = false, device = "", grad_x = false, grad_y = false}
	tf.BiasAdd(tensor<?x31542xf16>, tensor<31542xf16>) -> (tensor<?x31542xf16>) : {data_format = "NHWC", device = ""}
	tf.BiasAdd(tensor<?x678x1080xf16>, tensor<1080xf16>) -> (tensor<?x678x1080xf16>) : {data_format = "NHWC", device = ""}
	tf.BiasAdd(tensor<?x678x360xf16>, tensor<360xf16>) -> (tensor<?x678x360xf16>) : {data_format = "NHWC", device = ""}
	tf.ConcatV2(tensor<?x3x678x60xf16>, tensor<?x3x678x60xf16>, tensor<i32>) -> (tensor<?x3x678x120xf16>) : {device = ""}
	tf.ConcatV2(tensor<?x678x120xf16>, tensor<?x678x120xf16>, tensor<?x678x120xf16>, tensor<i32>) -> (tensor<?x678x360xf16>) : {device = ""}
	tf.Conv2D(tensor<?x1x678x360xf16>, tensor<1x1x120x360xf16>) -> (tensor<?x1x678x360xf16>) : {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Cos(tensor<?x678x120xf16>) -> (tensor<?x678x120xf16>) : {device = ""}
	tf.Exp(tensor<?x3x678x1xf16>) -> (tensor<?x3x678x1xf16>) : {device = ""}
	tf.Exp(tensor<?x3x678x87xf16>) -> (tensor<?x3x678x87xf16>) : {device = ""}
	tf.GatherV2(tensor<31542x339xf16>, tensor<?x678xi32>, tensor<i32>) -> (tensor<?x678x339xf16>) : {batch_dims = 0 : i64, device = ""}
	tf.MatMul(tensor<?x360xf16>, tensor<31542x360xf16>) -> (tensor<?x31542xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}
	tf.Mean(tensor<?x678x360xf16>, tensor<i32>) -> (tensor<?x678x1xf16>) : {device = "", keep_dims = true}
	tf.Mul(tensor<?x3x678x1xf16>, tensor<?x3x678x87xf16>) -> (tensor<?x3x678x87xf16>) : {device = ""}
	tf.Mul(tensor<?x3x678x1xf16>, tensor<f16>) -> (tensor<?x3x678x1xf16>) : {device = ""}
	tf.Mul(tensor<?x3x678x60xf16>, tensor<1x1x678x60xf16>) -> (tensor<?x3x678x60xf16>) : {device = ""}
	tf.Mul(tensor<?x678x1080xf16>, tensor<?x678x1080xf16>) -> (tensor<?x678x1080xf16>) : {device = ""}
	tf.Mul(tensor<?x678x120xf16>, tensor<f16>) -> (tensor<?x678x120xf16>) : {device = ""}
	tf.Mul(tensor<?x678x360xf16>, tensor<?x678x1xf16>) -> (tensor<?x678x360xf16>) : {device = ""}
	tf.Mul(tensor<?x678x360xf16>, tensor<?x678x360xf16>) -> (tensor<?x678x360xf16>) : {device = ""}
	tf.RealDiv(tensor<?x3x678x120xf16>, tensor<?x3x678x1xf16>) -> (tensor<?x3x678x120xf16>) : {device = ""}
	tf.Rsqrt(tensor<?x678x1xf16>) -> (tensor<?x678x1xf16>) : {device = ""}
	tf.Sigmoid(tensor<?x678x1080xf16>) -> (tensor<?x678x1080xf16>) : {device = ""}
	tf.Sigmoid(tensor<?x678x360xf16>) -> (tensor<?x678x360xf16>) : {device = ""}
	tf.Split(tensor<i32>, tensor<?x3x678x120xf16>) -> (tensor<?x3x678x60xf16>, tensor<?x3x678x60xf16>) : {device = ""}
	tf.Square(tensor<?x3x678x120xf16>) -> (tensor<?x3x678x120xf16>) : {device = ""}
	tf.Square(tensor<?x678x360xf16>) -> (tensor<?x678x360xf16>) : {device = ""}
	tf.StridedSlice(tensor<?x678x360xf16>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<?x360xf16>) : {begin_mask = 5 : i64, device = "", ellipsis_mask = 0 : i64, end_mask = 5 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}
	tf.Sub(tensor<?x3x678x60xf16>, tensor<?x3x678x60xf16>) -> (tensor<?x3x678x60xf16>) : {device = ""}
	tf.Sum(tensor<?x3x678x120xf16>, tensor<i32>) -> (tensor<?x3x678x1xf16>) : {device = "", keep_dims = true}
	tf.Sum(tensor<?x3x678x87xf16>, tensor<i32>) -> (tensor<?x3x1x87xf16>) : {device = "", keep_dims = true}
	tf.Transpose(tensor<?x3x1x87xf16>, tensor<4xi32>) -> (tensor<?x3x87x1xf16>) : {device = ""}
	tf.Transpose(tensor<?x3x678x120xf16>, tensor<4xi32>) -> (tensor<?x678x3x120xf16>) : {device = ""}
	tf.Transpose(tensor<?x3x678x87xf16>, tensor<4xi32>) -> (tensor<?x3x87x678xf16>) : {device = ""}
	tf.Transpose(tensor<?x678x3x120xf16>, tensor<4xi32>) -> (tensor<?x3x678x120xf16>) : {device = ""}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
..
----------------------------------------------------------------------
Ran 16 tests in 51.271s

OK
